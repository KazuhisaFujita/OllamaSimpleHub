# 📝 プロジェクト要件定義書

## 1. プロジェクト概要

* **プロジェクト名**: シンプル・マルチエージェント・アンサンブルシステム
* **目的**: ユーザーからの単一プロンプトに対し、ローカルネットワーク上の複数のOllama LLM（ワーカー）から回答を並列取得する。その後、それらの回答を単一の高性能LLM（レビューワー）が評価・統合し、最も高品質な「最終回答」を生成してユーザーに返却する。
* **利用言語**: Python 3.10以降
* **主要フレームワーク**: FastAPI, httpx, uvicorn

---

## 2. システム構成要素

### 2.1 統合サーバー (Integration Server)
* 本プロジェクトで作成するPythonアプリケーション
* ユーザーからのAPIリクエストを受け付ける窓口
* 全エージェントへの指示と結果の集約を行う「司令塔」

### 2.2 ワーカー・エージェント (Worker Agents)
* ネットワーク上の各WSで稼働する**Ollamaサーバー群**（複数）
* それぞれ異なるLLM（例: `llama3:8b`, `mistral`, `deepseek-coder`）が割り当てられている
* 同じプロンプトに対して、異なる視点や専門性で回答を生成

### 2.3 レビュー・エージェント (Review Agent)
* ネットワーク上のWSで稼働する**単一のOllamaサーバー**
* システム内で利用可能な**最も高性能なLLM**（例: `llama3:70b`）が割り当てられている
* 複数のワーカーの回答を評価・統合し、最終回答を生成

---

## 3. 機能要件 (Functional Requirements)

### FR1: サーバー機能
* `FastAPI`を使用し、HTTPサーバーを起動すること
* JSON形式のリクエストとレスポンスを処理すること
* 標準的なRESTful APIの慣例に従うこと

### FR2: APIエンドポイント

#### 3.2.1 エンドポイント: `POST /generate`
単一のエンドポイントで、マルチエージェント処理を実行する。

**リクエスト (Input)**:
```json
{
  "prompt": "ユーザーからの質問テキスト"
}
```

**レスポンス (Output)**:
```json
{
  "final_answer": "レビューワーが生成した最終回答",
  "review_comment": "レビューワーによる各回答の評価",
  "worker_responses": [
    {
      "agent_name": "worker_A (llama3:8b)",
      "response": "ワーカーAの生の回答"
    },
    {
      "agent_name": "worker_B (deepseek-coder)",
      "response": "ワーカーBの生の回答"
    },
    {
      "agent_name": "worker_C (mistral)",
      "response": "エラー: タイムアウトしました"
    }
  ],
  "metadata": {
    "total_workers": 3,
    "successful_workers": 2,
    "failed_workers": 1,
    "processing_time_seconds": 12.5
  }
}
```

#### 3.2.2 エンドポイント: `GET /health`
サーバーの状態を確認する。

**レスポンス**:
```json
{
  "status": "ok",
  "timestamp": "2025-11-18T10:30:00Z"
}
```

#### 3.2.3 エンドポイント: `GET /agents`
現在設定されているエージェント一覧を取得する。

**レスポンス**:
```json
{
  "reviewer": {
    "name": "Reviewer (Llama 3 70B)",
    "model": "llama3:70b",
    "api_url": "http://192.168.1.100:11434/api/chat"
  },
  "workers": [
    {
      "name": "Worker A (Llama 3 8B)",
      "model": "llama3:8b",
      "api_url": "http://192.168.1.101:11434/api/chat"
    },
    {
      "name": "Worker B (DeepSeek Coder)",
      "model": "deepseek-coder",
      "api_url": "http://192.168.1.102:11434/api/chat"
    }
  ]
}
```

### FR3: エージェント管理

#### 3.3.1 設定ファイルの読み込み
* ワーカー・エージェントおよびレビュー・エージェントのOllama APIエンドポイント（URLとモデル名）を、外部の設定ファイル（`config.json`）から読み込めること
* **目的**: コードを再起動せずにエージェントの追加・削除・変更を可能にするため

**config.json の例**:
```json
{
  "reviewer_agent": {
    "name": "Reviewer (Llama 3 70B)",
    "api_url": "http://192.168.1.100:11434/api/chat",
    "model": "llama3:70b",
    "timeout": 120
  },
  "worker_agents": [
    {
      "name": "Worker A (Llama 3 8B)",
      "api_url": "http://192.168.1.101:11434/api/chat",
      "model": "llama3:8b",
      "timeout": 60
    },
    {
      "name": "Worker B (DeepSeek Coder)",
      "api_url": "http://192.168.1.102:11434/api/chat",
      "model": "deepseek-coder",
      "timeout": 60
    },
    {
      "name": "Worker C (Mistral)",
      "api_url": "http://192.168.1.103:11434/api/chat",
      "model": "mistral",
      "timeout": 60
    }
  ],
  "system_settings": {
    "max_retries": 1,
    "default_timeout": 60,
    "stream": false
  }
}
```

#### 3.3.2 設定ファイルのバリデーション
* サーバー起動時に設定ファイルの形式が正しいかチェックすること
* 必須項目（`name`, `api_url`, `model`）が存在しない場合はエラーを出力すること

### FR4: 処理フロー

統合サーバーは、以下の順序で処理を実行すること：

1. **リクエスト受信**: `POST /generate` でリクエストを受信
2. **並列処理**: `config.json` に記載されている**全ワーカー・エージェント**に対し、Ollama APIリクエストを**非同期（並列）**で送信（`httpx.AsyncClient`と`asyncio.gather`の使用を推奨）
3. **結果収集**: 全ワーカーからのレスポンス（またはエラー）を収集
4. **プロンプト生成**: レビュー用プロンプトを生成（FR5で詳述）
5. **レビュー実行**: `config.json` に記載されている**レビュー・エージェント**に対し、Ollama APIリクエストを送信
6. **レスポンス解析**: レビュー・エージェントからのレスポンスを解析
7. **最終レスポンス返却**: （FR2で定義した）最終レスポンス（JSON）をクライアントに返却

#### 3.4.1 処理フローの詳細図
```
[User] 
  │
  ↓ POST /generate {"prompt": "..."}
[Integration Server]
  │
  ├─→ [Worker A (Llama3:8b)]     ──┐
  ├─→ [Worker B (DeepSeek)]      ──┤ (並列実行)
  └─→ [Worker C (Mistral)]       ──┘
  │
  ↓ 全ワーカーの回答を収集
  │
  ↓ レビュープロンプトを生成
  │
  └─→ [Reviewer (Llama3:70b)]
  │
  ↓ 最終回答を解析・整形
  │
  ↓ JSON レスポンス
[User]
```

### FR5: レビュープロンプト生成

#### 3.5.1 プロンプトテンプレート
* システム内部で、レビュー・エージェントに投げるためのプロンプトを動的に生成すること
* このプロンプトには、**①元のユーザーの質問** と **②全ワーカーの回答** が含まれている必要がある

**プロンプトテンプレートの例**:
```text
あなたはAIのチーフ・レビューワーです。以下のユーザーの質問に対し、複数のAIワーカーが回答しました。

# ユーザーの質問:
{user_prompt}

# ワーカーの回答群:
---
[Agent: {worker_A_name}]
{worker_A_response}
---
[Agent: {worker_B_name}]
{worker_B_response}
---
[Agent: {worker_C_name}]
{worker_C_response}
---

# あなたのタスク:
1. 【評価】: まず、各ワーカーの回答を簡潔に評価してください。
2. 【統合】: 次に、上記すべての回答を参考にし、誤りを正し、良い点を組み合わせて、単一の、最も高品質で完璧な「最終回答」を生成してください。

# 出力フォーマット（厳守）:
## 評価
（ここに評価を記述）

## 最終回答
（ここに統合した回答を記述）
```

#### 3.5.2 エラーハンドリング
* ワーカーがエラーを返した場合も、そのエラー内容をレビュープロンプトに含めること
* レビューワーには「このワーカーはエラーを返しました」という情報を明示すること

### FR6: エラーハンドリング

#### 3.6.1 ワーカーのエラー
* 特定のワーカーがタイムアウト、接続エラー、またはその他のエラーを返した場合:
  * そのワーカーの回答を`"エラー: {エラー内容}"`として記録
  * 他のワーカーの処理を継続
  * レスポンスの`worker_responses`にエラー情報を含める

#### 3.6.2 レビューワーのエラー
* レビューワーがエラーを返した場合:
  * HTTPステータスコード500を返却
  * エラーの詳細をレスポンスに含める
  * ワーカーの回答は保持し、ユーザーに返却

#### 3.6.3 設定ファイルのエラー
* `config.json`が存在しない、または形式が不正な場合:
  * サーバー起動時にエラーを出力
  * サーバーを起動しない

### FR7: ロギング

#### 3.7.1 ログレベル
* INFO: 通常の処理フロー（リクエスト受信、エージェント呼び出し、レスポンス返却）
* WARNING: 個別のワーカーのエラー、タイムアウト
* ERROR: レビューワーのエラー、システム全体に影響するエラー
* DEBUG: 詳細なリクエスト/レスポンス内容（開発時のみ）

#### 3.7.2 ログ出力内容
* タイムスタンプ
* ログレベル
* エージェント名（該当する場合）
* 処理内容の説明
* エラー内容（該当する場合）

---

## 4. 非機能要件 (Non-Functional Requirements)

### NFR1: 安定性（堅牢性）
* 特定のワーカー・エージェント（Ollamaサーバー）がオフライン、またはリクエストに失敗（タイムアウト等）しても、統合サーバー全体がクラッシュしないこと
* エラーを適切にハンドリングし、（FR2の例のように）レスポンスにエラー内容を含めること
* 少なくとも1つのワーカーが成功すれば、レビュー処理を続行できること

### NFR2: パフォーマンス
* ワーカー・エージェントへのリクエストは**並列処理**され、逐次処理（直列処理）であってはならない
* タイムアウト設定により、遅いワーカーが全体の処理をブロックしないこと
* **目標処理時間**: 3つのワーカー（各30秒）+ レビューワー（60秒）= 最大90秒以内

### NFR3: 拡張性
* 新しいワーカー・エージェントを追加する際、Pythonのコードを修正する必要がなく、`config.json`への追記のみで対応可能であること
* ワーカー数の上限は設けないが、10個程度まで対応できる設計とすること
* 将来的に、異なるLLMプロバイダー（OpenAI, Anthropic等）にも対応可能な設計とすること

### NFR4: 依存関係
* 外部の複雑なエージェント・フレームワーク（LangChain, AutoGen等）に依存せず、`fastapi`, `httpx`, `uvicorn` といった最小限のライブラリで完結すること
* **必須ライブラリ**:
  * `fastapi`: Webフレームワーク
  * `httpx`: 非同期HTTPクライアント
  * `uvicorn`: ASGIサーバー
  * `pydantic`: データバリデーション
* **オプションライブラリ**:
  * `python-dotenv`: 環境変数管理（将来的な拡張用）

### NFR5: 保守性
* コードは明確にモジュール化され、以下の責務に分割されること:
  * **APIルーター**: FastAPIのエンドポイント定義
  * **エージェント管理**: 設定ファイルの読み込み、エージェントへのリクエスト送信
  * **プロンプト生成**: レビュープロンプトの動的生成
  * **レスポンス解析**: レビューワーのレスポンスをパース
* 各モジュールは単体テスト可能であること

### NFR6: セキュリティ
* 設定ファイルにAPIキーや認証情報が必要な場合は、環境変数または別の秘密情報管理ツールを使用すること
* 本バージョンではローカルネットワーク内での使用を想定し、高度な認証機能は実装しない（将来的な拡張として残す）

---

## 5. システム制約

### 5.1 前提条件
* すべてのOllamaサーバーは、統合サーバーからネットワーク経由でアクセス可能であること
* 各Ollamaサーバーには、指定されたモデル（例: `llama3:8b`）が事前にダウンロード済みであること
* 統合サーバーを実行する環境は、Python 3.10以降がインストールされていること

### 5.2 環境要件
* **OS**: macOS, Linux, Windows（WSL2含む）
* **Python**: 3.10以降
* **メモリ**: 最小2GB（統合サーバー自体は軽量だが、並列リクエスト処理のため）
* **ネットワーク**: ローカルネットワーク（LAN）内での通信を想定

### 5.3 Ollama API仕様
* Ollamaの`/api/chat`エンドポイントを使用
* リクエスト形式:
  ```json
  {
    "model": "llama3:8b",
    "messages": [
      {"role": "user", "content": "質問テキスト"}
    ],
    "stream": false
  }
  ```
* レスポンス形式:
  ```json
  {
    "message": {
      "role": "assistant",
      "content": "回答テキスト"
    }
  }
  ```

---

## 6. 実装の優先順位

### フェーズ1: 基本機能（MVP）
1. FastAPIサーバーの起動
2. `config.json`の読み込み
3. 単一ワーカーへのリクエスト送信とレスポンス取得
4. `POST /generate`エンドポイントの実装（レビューワーなし）

### フェーズ2: マルチエージェント機能
1. 複数ワーカーへの並列リクエスト実装
2. エラーハンドリングの実装
3. レビュープロンプト生成機能
4. レビューワーの統合

### フェーズ3: 改善と拡張
1. `GET /health`と`GET /agents`エンドポイントの追加
2. ロギング機能の強化
3. メタデータ（処理時間、成功/失敗カウント）の追加
4. ドキュメント作成（API仕様、使用例）

---

## 7. テストケース

### 7.1 正常系
* **TC1**: 全ワーカーとレビューワーが正常に応答する場合
* **TC2**: ユーザープロンプトが空文字列の場合の挙動
* **TC3**: 複数の連続リクエストでの動作確認

### 7.2 異常系
* **TC4**: 1つのワーカーがタイムアウトした場合
* **TC5**: 全ワーカーがエラーを返した場合
* **TC6**: レビューワーがエラーを返した場合
* **TC7**: `config.json`が不正な形式の場合

### 7.3 性能テスト
* **TC8**: 10個のワーカーを並列実行した場合の処理時間
* **TC9**: 同時に複数のクライアントからリクエストを受けた場合の挙動

---

## 8. 今後の拡張案

### 8.1 短期的な拡張
* ストリーミングレスポンスのサポート（ワーカーとレビューワーの回答を逐次返却）
* WebUIの追加（ブラウザから直接プロンプトを入力できる）
* ワーカーの重み付け機能（特定のワーカーの回答を優先）

### 8.2 中長期的な拡張
* 異なるLLMプロバイダー（OpenAI, Anthropic, Cohere等）のサポート
* 会話履歴の保持（マルチターン対話）
* ワーカーの動的な選択（プロンプトの内容に応じて最適なワーカーを選択）
* データベースへのログ保存と分析機能

---

## 9. 用語集

| 用語 | 説明 |
|------|------|
| **統合サーバー** | 本プロジェクトで作成するFastAPIアプリケーション |
| **ワーカー・エージェント** | 並列で実行される複数のOllamaサーバー |
| **レビュー・エージェント** | ワーカーの回答を評価・統合する単一の高性能LLM |
| **Ollama** | ローカルで動作するLLMランタイム |
| **アンサンブル** | 複数のモデルの出力を組み合わせて最終的な予測を行う手法 |
| **非同期処理** | 複数の処理を並列に実行し、全体の処理時間を短縮する手法 |

---

## 10. 参考資料

* [FastAPI公式ドキュメント](https://fastapi.tiangolo.com/)
* [httpx公式ドキュメント](https://www.python-httpx.org/)
* [Ollama API仕様](https://github.com/ollama/ollama/blob/main/docs/api.md)
* [Python asyncio公式ドキュメント](https://docs.python.org/ja/3/library/asyncio.html)

---

## 改訂履歴

| バージョン | 日付 | 変更内容 | 担当者 |
|------------|------|----------|--------|
| 1.0 | 2025-11-18 | 初版作成 | - |

---

**この要件定義書は、プロジェクトの進行に応じて更新されます。**
